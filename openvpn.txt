When installing openvpn on arch linux there is a conflict.

sudo pacman -S openvpn
resolving dependencies...
looking for conflicting packages...
:: systemd-libs and libsystemd are in conflict. Remove libsystemd? [y/N] y

Packages (3) libsystemd-239.0-2 [removal]  systemd-libs-241.93-1  openvpn-2.4.7-1

Total Download Size:   0.79 MiB
Total Installed Size:  2.67 MiB
Net Upgrade Size:      0.19 MiB

:: Proceed with installation? [Y/n] y


So I just said yes :)
Lets see what happens.

The result was fine, nothing broke as far as I can tell.

To run the client just run:
sudo openvpn --config /path/to/config.ovpn




What is a tun interface?
Packets sent by an operating system via a tun/tap device are delivered to a user-space program which attaches itself to the device. A user-space program may also pass packets into a tun/tap device. In this case the tun/tap device delivers (or “injects”) these packets to the operating-system network stack thus emulating their reception from an external source. tun/tap interfaces are software-only interfaces, meaning that they exist only in the kernel and, unlike regular network interfaces, they have no physical hardware component (and so there’s no physical wire connected to them).

You can think of a tun/tap interface as a regular network interface that, when the kernel decides that the moment has come to send data “on the wire”, instead sends data to some userspace program that is attached to the interface.


General overview of linux kernel networking. It uses netfilter architecture. Firewalls, routing and the like are implemented via hooks at various points in the protocol stack. The
protocol stack being the stack from OSI model layer 1 to layer 7. Where layer 1 is implemented in hardware, layers 2-4 in kernel and layer 5-7 in userspace.
https://netfilter.org/documentation/HOWTO/netfilter-hacking-HOWTO-3.html


Well lots of interesting info!

First of all openvpn dies in "fail open" mode. So one needs to implement a kill switch mechanism.
Otherwise the traffic will be exposed over the network! Why? Well if openvpn dies, then tun0 fails to send
then kernel will route trafic through another interface.

So what mechanisms are there?
Basically its the firewall. Block all LAN -> WAN traffic. Allow only LAN -> VPN traffic. Consider installing this firewall on the router together with openvpn client.
But if no router is availiable you can add failsafe mechanism anyway. Use local firewall.

For example install UFW on linux. Then deny all in/out traffic. Then allow only traffic on tun0 interface.
Then allow for all physical interface outgoing traffic on port 1194, to allow openvpn client to actually send out data on physical interface.

Then DNS is an issue. If you are connected by wifi, network manager will manage your /etc/resolv.conf. The nameserver in /etc/resolv.conf will typically be your
wifi access point (i.e. router) which has a local ip (like 192.168.1.1). This is a problem because when DNS traffic emerges from the other end of tun0 tunnel it of course
can not find the specified DNS server at 192.168.1.1. So either push a DNS setting via openvpn client config OR open up port 53 on physical interface. 
If pushing a setting consider googles DNS of 8.8.8.8 which has the disadvantage of being slow because its far away from you, or if OpenVPN server is also a
Bind9 server push its own ip address. Allowing outbound traffic on port 53, in my opinion ruins the point of routing everything over vpn.


Keep in mind that by default only IPv4 is tunneled. IPv6 needs special config on both client and server. The problem is that some ISP
(like russian mobile carrier MTS) provide IPv6, so when you are connected to such an ISP some applications will prefer IPv6.
This is a problem because IPv6 leaks into the open internet! In particular Firefox prefers to use IPv6 when it can! (see https://serverfault.com/questions/948051/openvpn-leaks-ipv6-in-ubuntu-but-not-in-android)

See this page for details of your IP and VPN functionallity: https://ipleak.net/

Also remember that DNS leaks might not sound like much, but if someone collects them thay can see the names of all the websites you visited. Because DNS stores address in plain text.

The decision whether to use IPv4 or 6 lies largely with the OS and the RFC for IPv6 specifies that IPv6 should be prefered.
see: https://askubuntu.com/questions/9181/how-to-let-the-browser-prefer-ipv6-over-ipv4
So there will be more and more ipv6 around soon. Better make OpenVPN server support the ipv6 tunnel.
Trying to firewall IPv4 to force all traffic through IPv4 tunnel seems short-sighted.

But! Not all hosting companies allow ipv6. And on some companies you have to edit /etc/network/interfaces to bring up the <global> ipv6 portion of the interface.. The ipv6 <link> interface is ususally there for most ihardware nterfaces.
How to edit /etc/network/interfaces to bring it up see: https://www.cyberciti.biz/faq/ubuntu-ipv6-networking-configuration/

Here is an example stanza:
# IPv6 interface
iface ens18 inet6 static
	address 2a00:b700:a::6:221
	netmask 64
	gateway 2a00:b700:a::1
	dns-nameservers 2a00:b700:a::220 2a00:b700:a:1::220

More about IPv6!
So by default every client in IPv6 network gets a /64 bits subnet. In case of configuring OpenVPN docker container we should split this
subnet in two to allow some for docker containers and some for what ever other uses.
see: https://docs.docker.com/v17.09/engine/userguide/networking/default_network/ipv6/

When you buy a vps the provider gives you a specific ipv6 address. However to comply with RFC they actually give you the whole subnet /64.
see: https://www.lowendtalk.com/discussion/27455/yes-you-can-use-the-whole-64-of-ipv6-on-ovhs-kimsufi
So you can actually ping the next address in the subnetwork that you were given out and reach it from the outside.

To understand how ipv6 allocation works see this IBM article:
https://www.ibm.com/support/knowledgecenter/en/STCMML8/com.ibm.storage.ts3500.doc/opg_3584_IPv4_IPv6_prefix_subnet_mask.html

When running openvpn docker container you must allocate a subnet of IPv6 to docker. Also know that docker documentation is currently wrong and just
enabling "ipv6":true in daemon.json will not work. See: https://github.com/moby/moby/issues/36954

So how to subnet then? Well docker needs at least /80 so give it to him. Lets say the ip address give to us by the vps provider is 2a00:b700:a::6:221 that means we actually have addresses:
from 2a00:b700:000a:0000:0000:0000:0000:0000
upto 2a00:b700:000a:0000:FFFF:FFFF:FFFF:FFFF
We will give docker a /80 subnet but prefix it with a 1 like this 2a00:b700:a:0:1::0/80 which means addresses:
from 2a00:b700:000a:0000:0001:0000:0000:0000
upto 2a00:b700:000a:0000:0001:FFFF:FFFF:FFFF

Well it was a long journey!

So I setup openvpn and all was well. Then I opened wireshark and noticed that if ipv6 was used the packets did not route through OpenVPN. They leaked!
That means that when you are by chance connected to an ipv6 enabled network all your traffic would leak, because actually modern OS prefere to use ipv6.
This had to be fixed! And the way to fix it was of course to enable ipv4 and ipv6 runneling in OpenVPN. Because ipv6 is on the rollout everywhere!

First of all lets back up some claims I make. Modern OS try to evaluate which is faster ipv4 or ipv6 and use that. They keyhole is the getaddrinfo function which
returns ipv6, ipv4 or both. The OS manipulates its return values to either provide the application with only ipv4, or only ipv6 or both (leaving up to the application to choose).
The getaddrinfo function can also be manipulated by /etc/gai.conf By editing that file you can make the OS prefer ipv4 over ipv6. But the default is ipv6.

So then I was off on a journey. To be honest most of the dance was around the ipv6 ISP and docker. The openvpn docker image was ready for ipv6.
But some adjustments had to be made. So talking about server openvpn config file. This is created by ovpn_genconfig script. Its important to understand
the difference between "route" and "push route" (a "route" directive takes a subnet (ip and netmask) upon an arrival of a package for this
subnet the server will put it on the tun device). Also its important to see how ipv4 is done and just repeat the steps for ipv6. For ipv4 there is
a "server" directive which specifies a subnet (ip and netmask).

So in case of ipv6 I created a special subnet of ipv6 addresses (a /64 subnet, because my vps provider gave me a /48). I set the server-ipv6 directive
to describe that subnet (ip and prefix length). Finally I added a 'push "route-ipv6 2000::/3"' so clients will route all internet traffic over tun0
interface.

Its really funny how there words like "route" sound so alien until you begin to understand it. Now this stuff is just so obvious.
It feels like I spend the last week mulling over things that are so obvious, clear and simple :)

Know that IPv6 are meant to be unique. So if your device is offline or online or its a local network in a random field that you have
setup, it does not matter, but the ipv6 is unique! When that device which is now in a local network in some field finally goes online
it will be the sole owner of that address, and I as the owner of the prefix have to guarantee that that device is the sole owner.

Some questions were raised as to how docker gets involved into the whole ipv6 thing. Well docker needs to have ipv6 enabled and needs
a subnet to assign ipv6 addresses to containers.

Here is how it looks like when ISP supports ipv6 and everything is ipv6:
Sun Jun 23 02:53:28 2019 library versions: OpenSSL 1.1.1b  26 Feb 2019, LZO 2.10
Sun Jun 23 02:53:28 2019 TCP/UDP: Preserving recently used remote address: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 02:53:28 2019 UDPv6 link local: (not bound)
Sun Jun 23 02:53:28 2019 UDPv6 link remote: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 02:53:29 2019 [ansible.rinserepeat.site] Peer Connection Initiated with [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 02:53:30 2019 Options error: Unrecognized option or missing or extra parameter(s) in [PUSH-OPTIONS]:1: block-outside-dns (2.4.7)
Sun Jun 23 02:53:30 2019 TUN/TAP device tun0 opened
Sun Jun 23 02:53:30 2019 /usr/bin/ip link set dev tun0 up mtu 1500
Sun Jun 23 02:53:30 2019 /usr/bin/ip addr add dev tun0 local 192.168.255.10 peer 192.168.255.9
Sun Jun 23 02:53:30 2019 /usr/bin/ip -6 addr add 2a00:b700:0:feed::1001/64 dev tun0
Sun Jun 23 02:53:30 2019 add_route_ipv6(2a00:b700::6:220/128 -> fe80::9e5c:f9ff:fed4:8d2d metric 1) dev wlo1
Sun Jun 23 02:53:30 2019 add_route_ipv6(2000::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 02:53:30 2019 add_route_ipv6(::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 02:53:30 2019 add_route_ipv6(2000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 02:53:30 2019 add_route_ipv6(3000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 02:53:30 2019 add_route_ipv6(fc00::/7 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this
Sun Jun 23 03:01:02 2019 Initialization Sequence Completed

Here is how it looks like when your ISP does not support ipv6 (it falls back to ipv4):
Sun Jun 23 03:00:56 2019 TCP/UDP: Preserving recently used remote address: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 03:00:56 2019 UDPv6 link local: (not bound)
Sun Jun 23 03:00:56 2019 UDPv6 link remote: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 03:00:56 2019 write UDPv6: Network is unreachable (code=101)
Sun Jun 23 03:00:56 2019 Network unreachable, restarting
Sun Jun 23 03:00:56 2019 SIGUSR1[soft,network-unreachable] received, process restarting
Sun Jun 23 03:01:01 2019 TCP/UDP: Preserving recently used remote address: [AF_INET]185.22.153.49:1194
Sun Jun 23 03:01:01 2019 UDP link local: (not bound)
Sun Jun 23 03:01:01 2019 UDP link remote: [AF_INET]185.22.153.49:1194
Sun Jun 23 03:01:01 2019 [ansible.rinserepeat.site] Peer Connection Initiated with [AF_INET]185.22.153.49:1194
Sun Jun 23 03:01:02 2019 Options error: Unrecognized option or missing or extra parameter(s) in [PUSH-OPTIONS]:1: block-outside-dns (2.4.7)
Sun Jun 23 03:01:02 2019 TUN/TAP device tun0 opened
Sun Jun 23 03:01:02 2019 /usr/bin/ip link set dev tun0 up mtu 1500
Sun Jun 23 03:01:02 2019 /usr/bin/ip addr add dev tun0 local 192.168.255.6 peer 192.168.255.5
Sun Jun 23 03:01:02 2019 /usr/bin/ip -6 addr add 2a00:b700:0:feed::1000/64 dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(2000::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(2000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(3000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(fc00::/7 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this
Sun Jun 23 03:01:02 2019 Initialization Sequence Completed


For completeness here is the server openvpn config:
server 192.168.255.0 255.255.255.0
verb 3
key /etc/openvpn/pki/private/mysite.key
ca /etc/openvpn/pki/ca.crt
cert /etc/openvpn/pki/issued/mysite.crt
dh /etc/openvpn/pki/dh.pem
tls-auth /etc/openvpn/pki/ta.key
key-direction 0
keepalive 10 60
persist-key
persist-tun

proto udp6
# Rely on Docker to do port mapping, internally always 1194
port 1194
dev tun0
status /tmp/openvpn-status.log

user nobody
group nogroup
client-to-client
comp-lzo

### Route Configurations Below

### Push Configurations Below
push "block-outside-dns"
push "dhcp-option DNS 8.8.8.8"
push "dhcp-option DNS 8.8.4.4"

### Extra Configurations Below
server-ipv6 2a00:b700:a:feed::/64
push "route-ipv6 2000::/3"
duplicate-cn

When the server is starting it has the following lines which describe its ip address pools:
Sat Jun 22 23:16:18 2019 IFCONFIG POOL IPv6: (IPv4) size=62, size_ipv6=65536, netbits=64, base_ipv6=2a00:b700:a:feed::1000
Sat Jun 22 23:16:18 2019 IFCONFIG POOL: base=192.168.255.4 size=62, ipv6=1


So in the end I ran some tests.
First I connected to an ipv6 enabled ISP and connected to my OpenVPN. All was ok.
