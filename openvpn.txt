When installing openvpn on arch linux there is a conflict.

sudo pacman -S openvpn
resolving dependencies...
looking for conflicting packages...
:: systemd-libs and libsystemd are in conflict. Remove libsystemd? [y/N] y

Packages (3) libsystemd-239.0-2 [removal]  systemd-libs-241.93-1  openvpn-2.4.7-1

Total Download Size:   0.79 MiB
Total Installed Size:  2.67 MiB
Net Upgrade Size:      0.19 MiB

:: Proceed with installation? [Y/n] y


So I just said yes :)
Lets see what happens.

The result was fine, nothing broke as far as I can tell.

To run the client just run:
sudo openvpn --config /path/to/config.ovpn

Network fundamentals: http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.html


What is a tun interface?
Packets sent by an operating system via a tun/tap device are delivered to a user-space program which attaches itself to the device. A user-space program may also pass packets into a tun/tap device. In this case the tun/tap device delivers (or “injects”) these packets to the operating-system network stack thus emulating their reception from an external source. tun/tap interfaces are software-only interfaces, meaning that they exist only in the kernel and, unlike regular network interfaces, they have no physical hardware component (and so there’s no physical wire connected to them).

You can think of a tun/tap interface as a regular network interface that, when the kernel decides that the moment has come to send data “on the wire”, instead sends data to some userspace program that is attached to the interface.


General overview of linux kernel networking. It uses netfilter architecture. Firewalls, routing and the like are implemented via hooks at various points in the protocol stack. The
protocol stack being the stack from OSI model layer 1 to layer 7. Where layer 1 is implemented in hardware, layers 2-4 in kernel and layer 5-7 in userspace.
https://netfilter.org/documentation/HOWTO/netfilter-hacking-HOWTO-3.html


Well lots of interesting info!

First of all openvpn dies in "fail open" mode. So one needs to implement a kill switch mechanism.
Otherwise the traffic will be exposed over the network! Why? Well if openvpn dies, then tun0 fails to send
then kernel will route trafic through another interface.

So what mechanisms are there?
Basically its the firewall. Block all LAN -> WAN traffic. Allow only LAN -> VPN traffic. Consider installing this firewall on the router together with openvpn client.
But if no router is availiable you can add failsafe mechanism anyway. Use local firewall.

For example install UFW on linux. Then deny all in/out traffic. Then allow only traffic on tun0 interface.
Then allow for all physical interface outgoing traffic on port 1194, to allow openvpn client to actually send out data on physical interface.

Then DNS is an issue. If you are connected by wifi, network manager will manage your /etc/resolv.conf. The nameserver in /etc/resolv.conf will typically be your
wifi access point (i.e. router) which has a local ip (like 192.168.1.1). This is a problem because when DNS traffic emerges from the other end of tun0 tunnel it of course
can not find the specified DNS server at 192.168.1.1. So either push a DNS setting via openvpn client config OR open up port 53 on physical interface.
If pushing a setting consider googles DNS of 8.8.8.8 which has the disadvantage of being slow because its far away from you, or if OpenVPN server is also a
Bind9 server push its own ip address. Allowing outbound traffic on port 53, in my opinion ruins the point of routing everything over vpn.

A note about the push directive.
This only gets into the client if the client config has a pull directive. Pull option must be used on a client which is connecting to a multi-client server.
It indicates to OpenVPN that it should accept options pushed by the server, (note that the -pull option is implied by -client ).In particular, -pull allows the
server to push routes to the client, so you should not use -pull or -client in situations where you don’t trust the server to have control over the client’s routing table.
In general its best to push as much as possible vs writing directly into a client config. Then the trusted server is the single point of truth (SPOT).

Keep in mind that by default only IPv4 is tunneled. IPv6 needs special config on both client and server. The problem is that some ISP
(like russian mobile carrier MTS) provide IPv6, so when you are connected to such an ISP some applications will prefer IPv6.
This is a problem because IPv6 leaks into the open internet! In particular Firefox prefers to use IPv6 when it can! (see https://serverfault.com/questions/948051/openvpn-leaks-ipv6-in-ubuntu-but-not-in-android)

See this page for details of your IP and VPN functionallity: https://ipleak.net/

Also remember that DNS leaks might not sound like much, but if someone collects them thay can see the names of all the websites you visited. Because DNS stores address in plain text.

The decision whether to use IPv4 or 6 lies largely with the OS and the RFC for IPv6 specifies that IPv6 should be prefered.
see: https://askubuntu.com/questions/9181/how-to-let-the-browser-prefer-ipv6-over-ipv4
So there will be more and more ipv6 around soon. Better make OpenVPN server support the ipv6 tunnel.
Trying to firewall IPv4 to force all traffic through IPv4 tunnel seems short-sighted.

But! Not all hosting companies allow ipv6. And on some companies you have to edit /etc/network/interfaces to bring up the <global> ipv6 portion of the interface.. The ipv6 <link> interface is ususally there for most ihardware nterfaces.
How to edit /etc/network/interfaces to bring it up see: https://www.cyberciti.biz/faq/ubuntu-ipv6-networking-configuration/

Here is an example stanza:
# IPv6 interface
iface ens18 inet6 static
	address 2a00:b700:a::6:221
	netmask 64
	gateway 2a00:b700:a::1
	dns-nameservers 2a00:b700:a::220 2a00:b700:a:1::220

More about IPv6!
So by default every client in IPv6 network gets a /64 bits subnet. In case of configuring OpenVPN docker container we should split this
subnet in two to allow some for docker containers and some for what ever other uses.
see: https://docs.docker.com/v17.09/engine/userguide/networking/default_network/ipv6/

When you buy a vps the provider gives you a specific ipv6 address. However to comply with RFC they actually give you the whole subnet /64.
see: https://www.lowendtalk.com/discussion/27455/yes-you-can-use-the-whole-64-of-ipv6-on-ovhs-kimsufi
So you can actually ping the next address in the subnetwork that you were given out and reach it from the outside.

To understand how ipv6 allocation works see this IBM article:
https://www.ibm.com/support/knowledgecenter/en/STCMML8/com.ibm.storage.ts3500.doc/opg_3584_IPv4_IPv6_prefix_subnet_mask.html

When running openvpn docker container you must allocate a subnet of IPv6 to docker. Also know that docker documentation is currently wrong and just
enabling "ipv6":true in daemon.json will not work. See: https://github.com/moby/moby/issues/36954

So how to subnet then? Well docker needs at least /80 so give it to him. Lets say the ip address give to us by the vps provider is 2a00:b700:a::6:221 that means we actually have addresses:
from 2a00:b700:000a:0000:0000:0000:0000:0000
upto 2a00:b700:000a:0000:FFFF:FFFF:FFFF:FFFF
We will give docker a /80 subnet but prefix it with a 1 like this 2a00:b700:a:0:1::0/80 which means addresses:
from 2a00:b700:000a:0000:0001:0000:0000:0000
upto 2a00:b700:000a:0000:0001:FFFF:FFFF:FFFF

Well it was a long journey!

So I setup openvpn and all was well. Then I opened wireshark and noticed that if ipv6 was used the packets did not route through OpenVPN. They leaked!
That means that when you are by chance connected to an ipv6 enabled network all your traffic would leak, because actually modern OS prefere to use ipv6.
This had to be fixed! And the way to fix it was of course to enable ipv4 and ipv6 runneling in OpenVPN. Because ipv6 is on the rollout everywhere!

First of all lets back up some claims I make. Modern OS try to evaluate which is faster ipv4 or ipv6 and use that. They keyhole is the getaddrinfo function which
returns ipv6, ipv4 or both. The OS manipulates its return values to either provide the application with only ipv4, or only ipv6 or both (leaving up to the application to choose).
The getaddrinfo function can also be manipulated by /etc/gai.conf By editing that file you can make the OS prefer ipv4 over ipv6. But the default is ipv6.

So then I was off on a journey. To be honest most of the dance was around the ipv6 ISP and docker. The openvpn docker image was ready for ipv6.
But some adjustments had to be made. So talking about server openvpn config file. This is created by ovpn_genconfig script. Its important to understand
the difference between "route" and "push route" (a "route" directive takes a subnet (ip and netmask) upon an arrival of a package for this
subnet the server will put it on the tun device). Also its important to see how ipv4 is done and just repeat the steps for ipv6. For ipv4 there is
a "server" directive which specifies a subnet (ip and netmask).

So in case of ipv6 I created a special subnet of ipv6 addresses (a /64 subnet, because my vps provider gave me a /48). I set the server-ipv6 directive
to describe that subnet (ip and prefix length). Finally I added a 'push "route-ipv6 2000::/3"' so clients will route all internet traffic over tun0
interface.

So to be clear: an openvpn server gets a subnet (network A) from which it allocates ips for clients. Now some clients (client A) just use this subnet,
but some clients (client B) are gateways (router) to another network (network B). So this network B uses a different subnet from what the OpenVPN
server has. We can instruct the OpenVPN server that when it gets a packet destined to network B it will send it through tun0 interface (which means
that client B will be able to pick it up).
This is used if you have a network B which is ipv4 only and you want to access the internet via ipv6, so you tunnel to OpenVPN which has ipv6 enabled.
See: https://openoffice.nl/2018/04/05/ipv6-openvpn-part2/

Its really funny how there words like "route" sound so alien until you begin to understand it. Now this stuff is just so obvious.
It feels like I spend the last week mulling over things that are so obvious, clear and simple :)

Know that IPv6 are meant to be unique. So if your device is offline or online or its a local network in a random field that you have
setup, it does not matter, but the ipv6 is unique! When that device which is now in a local network in some field finally goes online
it will be the sole owner of that address, and I as the owner of the prefix have to guarantee that that device is the sole owner.

Some questions were raised as to how docker gets involved into the whole ipv6 thing. Well docker needs to have ipv6 enabled and needs
a subnet to assign ipv6 addresses to containers.

Here is how it looks like when ISP supports ipv6 and everything is ipv6:
Sun Jun 23 13:14:20 2019 TCP/UDP: Preserving recently used remote address: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 13:14:20 2019 UDPv6 link local: (not bound)
Sun Jun 23 13:14:20 2019 UDPv6 link remote: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 13:14:20 2019 [ansible.mysite.site] Peer Connection Initiated with [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 13:14:21 2019 TUN/TAP device tun0 opened
Sun Jun 23 13:14:21 2019 /usr/bin/ip link set dev tun0 up mtu 1500
Sun Jun 23 13:14:21 2019 /usr/bin/ip addr add dev tun0 local 192.168.255.6 peer 192.168.255.5
Sun Jun 23 13:14:21 2019 /usr/bin/ip -6 addr add 2a00:b700:0:feed::1000/64 dev tun0
Sun Jun 23 13:14:21 2019 add_route_ipv6(2a00:b700::6:220/128 -> fe80::9e5c:f9ff:fed4:8d2d metric 1) dev wlo1
Sun Jun 23 13:14:21 2019 add_route_ipv6(2000::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 13:14:21 2019 add_route_ipv6(::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 13:14:21 2019 add_route_ipv6(2000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 13:14:21 2019 add_route_ipv6(3000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 13:14:21 2019 add_route_ipv6(fc00::/7 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 13:14:21 2019 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this
Sun Jun 23 13:14:21 2019 Initialization Sequence Completed

Here is how it looks like when your ISP does not support ipv6 (it falls back to ipv4):
Sun Jun 23 03:00:56 2019 TCP/UDP: Preserving recently used remote address: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 03:00:56 2019 UDPv6 link local: (not bound)
Sun Jun 23 03:00:56 2019 UDPv6 link remote: [AF_INET6]2a00:b700::6:220:1194
Sun Jun 23 03:00:56 2019 write UDPv6: Network is unreachable (code=101)
Sun Jun 23 03:00:56 2019 Network unreachable, restarting
Sun Jun 23 03:00:56 2019 SIGUSR1[soft,network-unreachable] received, process restarting
Sun Jun 23 03:01:01 2019 TCP/UDP: Preserving recently used remote address: [AF_INET]185.22.153.49:1194
Sun Jun 23 03:01:01 2019 UDP link local: (not bound)
Sun Jun 23 03:01:01 2019 UDP link remote: [AF_INET]185.22.153.49:1194
Sun Jun 23 03:01:01 2019 [ansible.mysite.site] Peer Connection Initiated with [AF_INET]185.22.153.49:1194
Sun Jun 23 03:01:02 2019 Options error: Unrecognized option or missing or extra parameter(s) in [PUSH-OPTIONS]:1: block-outside-dns (2.4.7)
Sun Jun 23 03:01:02 2019 TUN/TAP device tun0 opened
Sun Jun 23 03:01:02 2019 /usr/bin/ip link set dev tun0 up mtu 1500
Sun Jun 23 03:01:02 2019 /usr/bin/ip addr add dev tun0 local 192.168.255.6 peer 192.168.255.5
Sun Jun 23 03:01:02 2019 /usr/bin/ip -6 addr add 2a00:b700:0:feed::1000/64 dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(2000::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(::/3 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(2000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(3000::/4 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 add_route_ipv6(fc00::/7 -> 2a00:b700:0:feed::1 metric -1) dev tun0
Sun Jun 23 03:01:02 2019 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this
Sun Jun 23 03:01:02 2019 Initialization Sequence Completed


For completeness here is the server openvpn config:
"""
server 192.168.255.0 255.255.255.0
verb 3
key /etc/openvpn/pki/private/ansible.mysite.site.key
ca /etc/openvpn/pki/ca.crt
cert /etc/openvpn/pki/issued/ansible.mysite.site.crt
dh /etc/openvpn/pki/dh.pem
tls-auth /etc/openvpn/pki/ta.key
key-direction 0
keepalive 10 60
persist-key
persist-tun

proto udp6
# Rely on Docker to do port mapping, internally always 1194
port 1194
dev tun0
status /tmp/openvpn-status.log

user nobody
group nogroup
client-to-client
comp-lzo

### Push Configurations Below
push "dhcp-option DNS 8.8.8.8"
push "dhcp-option DNS 8.8.4.4"

### Extra Configurations Below
server-ipv6 2a00:b700:0:feed::/64
push "route-ipv6 2000::/3"
duplicate-cn
"""

When the server is starting it has the following lines which describe its ip address pools:
Sat Jun 22 23:16:18 2019 IFCONFIG POOL IPv6: (IPv4) size=62, size_ipv6=65536, netbits=64, base_ipv6=2a00:b700:a:feed::1000
Sat Jun 22 23:16:18 2019 IFCONFIG POOL: base=192.168.255.4 size=62, ipv6=1

After your connection with openvpn server is established you can check it in various ways:
$ ip address show dev tun0
27: tun0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 100
    link/none
    inet 192.168.255.6 peer 192.168.255.5/32 scope global tun0
       valid_lft forever preferred_lft forever
    inet6 2a00:b700:0:feed::1000/64 scope global
       valid_lft forever preferred_lft forever
    inet6 fe80::31d4:af94:b35b:f967/64 scope link stable-privacy
       valid_lft forever preferred_lft forever

$ ip route
0.0.0.0/1 via 192.168.255.5 dev tun0
default via 192.168.43.1 dev wlo1 proto dhcp src 192.168.43.248 metric 303
128.0.0.0/1 via 192.168.255.5 dev tun0
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
192.168.43.0/24 dev wlo1 proto dhcp scope link src 192.168.43.248 metric 303
192.168.255.0/24 via 192.168.255.5 dev tun0
192.168.255.5 dev tun0 proto kernel scope link src 192.168.255.6

$ ip -6 route
::1 dev lo proto kernel metric 256 pref medium
::/3 dev tun0 metric 1024 pref medium
2a00:1fa1:c6a0:77de::/64 dev wlo1 proto ra metric 303 mtu 1500 pref medium
2a00:1fa1:c6a0:77de::/64 dev wlo1 proto ra metric 600 pref medium
2a00:b700::6:220 via fe80::9e5c:f9ff:fed4:8d2d dev wlo1 metric 1 pref medium
2a00:b700:0:feed::/64 dev tun0 proto kernel metric 256 pref medium
2000::/4 dev tun0 metric 1024 pref medium
3000::/4 dev tun0 metric 1024 pref medium
2000::/3 dev tun0 metric 1024 pref medium
fc00::/7 dev tun0 metric 1024 pref medium
fe80::/64 dev wlo1 proto kernel metric 256 pref medium
fe80::/64 dev tun0 proto kernel metric 256 pref medium
fe80::/64 dev wlo1 proto kernel metric 600 pref medium
default via fe80::9e5c:f9ff:fed4:8d2d dev wlo1 proto ra metric 303 mtu 1500 pref medium
default via fe80::9e5c:f9ff:fed4:8d2d dev wlo1 proto ra metric 600 pref high

So we can see that tun0 interface has an IPv6 and IPv4.
We can see that there is a top route that sends all ipv4 to tun0
We can see that there is a ipv6 route on wlo1 which is because I am connected to a hotspot with ipv6, so reaching
device on the LAN I would just use wlo1 route. But for ipv6 internet 2000::/3 everything goes to tun0.

Here is the client config:
"""
client
nobind
dev tun
remote-cert-tls server

remote ansible.mysite.site 1194 udp6
remote ansible.mysite.site 1194 udp
redirect-gateway def1 ipv6

<key>
-----BEGIN PRIVATE KEY-----

-----END PRIVATE KEY-----
</key>
<cert>
-----BEGIN CERTIFICATE-----

-----END CERTIFICATE-----
</cert>
<ca>
-----BEGIN CERTIFICATE-----

-----END CERTIFICATE-----
</ca>
key-direction 1
<tls-auth>
#
# 2048 bit OpenVPN static key
#
-----BEGIN OpenVPN Static key V1-----

-----END OpenVPN Static key V1-----
</tls-auth>

comp-lzo
"""

Note that the client has a remote with udp6 and just udp. It will try ipv6 first and then fallback to ipv4.
The "redirect-gateway ipv6" is necessary because there is a bug in iOS 9 and Android. So its not necessary for desktop, but will not hurt.

So in the end I ran some tests.
One: I connected to an ipv6 enabled ISP and connected to my OpenVPN. In wireshark I could see that DNS queries went normally, while all other traffic
went over OpenVPN and used ipv6.
Two: I connected to an ipv4 only ISP and connected to OpenVPN. In wireshark I could see that all traffic went over OpenVPN and used ipv4.

By default OpenVPN uses p2p topology. This is bad in many ways, see: https://community.openvpn.net/openvpn/wiki/Topology
You can read informally about p2p topology here: https://stackoverflow.com/questions/36375530/what-is-the-destination-address-for-a-tap-tun-device
The main thing is that Windows clients are not supported in p2p topology.
So we switch to subnet topology. After defining topology in the server config, this option is often pushed to
clients, see: https://community.openvpn.net/openvpn/wiki/Concepts-Addressing#Examples

If you can connect to your VPN and get "Initialisation sequence success" then you should be able to ping the server
and server should be able to ping you without setting up any routes, read the second message in forum
https://forums.openvpn.net/viewtopic.php?t=9202 for this claim. You will have a client <-> server working ok.
But you might have trouble with "client -> server -> Internet".
Because this step requiers extra configuration on the server side that is not provided by OpenVPN package. It is
provided by the docker openvpn image.
The real problem is this: traffic originates at your machine with VPN ip of 192.168.255.4, goes to VPN server, exists to
the internet, finds the target, then the target replies and sends the responce back to ip 192.168.255.4, which of course
will never return to your actual server (at ansible.mysite.site). So if you want it to return you must masquarade the outgoing
traffic, you rewrite the 192.168.255.4 ip with the ip of the ansible.mysite.site vps (where VPN server is hosted). Then traffic
will return back to VPN server and iptable rules will de-masquarade it and it will go down the vpn pipe back to 192.168.255.4.
You must get iptables NAT POSTROUTING MASQUERADE set up. Here is info from official OpenVPN docs:
https://openvpn.net/community-resources/how-to/#routing-all-client-traffic-including-web-traffic-through-the-vpn
Here is what other people who have not read the official doc and also have trouble with Internet connection are saying:
https://unix.stackexchange.com/questions/136211/routing-public-ipv6-traffic-through-openvpn-tunnel
https://forums.openvpn.net/viewtopic.php?t=21051
https://ubuntuforums.org/showthread.php?t=2378684
The docker openvpn image sets up iptables NAT POSTROUTING MASQUERADE with "setupIptablesAndRouting" function in "ovpn_run"
script. After the iptables nat rules are altered packet flow client -> server -> Internet and back works.
Specifically the POSTROUTING chain is altered. This chain is not shown with "iptables --list", to see it use
"iptables -t nat --list". To understand in more depth the whole NAT thing, first check out what is iptables NAT and
iptables MASQUERADE, see https://askubuntu.com/questions/466445/what-is-masquerade-in-the-context-of-iptables
and http://gsp.com/cgi-bin/man.cgi?section=3&topic=libalias just read the Introduction paragraph.


Here is a diagram (without any arrows) of my current setup:
                                                                              client tun0                        client tun0
                                                                              ip4: 192.168.255.2/24              ip4: 192.168.255.3/24
                                                                              ip6: 2a00:b700:0:feed::1000/64     ip6: 2a00:b700:0:feed::1001/64



                                                                                                 Gateway tun0
                                                                                                 ip4: 192.168.255.1/24
                                                                                                 ip6: 2a00:b700:0:feed::1/64

mtproto-proxy eth0                               bind9-dns eth0                                  openvpn-server eth0
ip4: 172.17.0.3/16                               ip4: 172.17.0.2/16                              ip4: 172.17.0.4/16
ip6: 2a00:b700:0:cafe:0:242:ac11:3/64            ip6: 2a00:b700:0:cafe:0:242:ac11:2/64           ip6: 2a00:b700:0:cafe:0:242:ac11:4/64



                                                     Gateway docker0
                                                     ip4: 172.17.0.1
                                                     ip6: 2a00:b700:0:cafe::1

                                                     VPS eth0
                                                     ip4: 185.22.153.49/23
                                                     ip6: 2a00:b700::6:220/48



                                                        Gateway
                                                        ip4: 185.22.152.1
                                                        ip6: 2a00:b700::1


The VPN itself is working. But there is another issue: doker does not let ipv6 out/in from 172.17.0.1 network.
This is outlined in more details here: https://serverfault.com/questions/943123/ping-public-ipv6-dns-from-docker-container
And slightly less similar issue here: https://serverfault.com/questions/782920/no-ipv6-connectivity-from-docker-container

First of all I changed the subnet for ipv6 VPN, now its fc00:0:0:feed::1/64. This change should not really affect routing
but has the benefit that if traffic escapes it will not be routed properly and will just die. Also this covers a
possible hole: if this traffic gets out of openvpn-server eth0 then before going out to the internet its ULA ip6 address
will trigger it to pass through ipv6nat container, which is what we want.
After also installing ipv6nat docker image the situation looks as follows:

                                                                              client tun0                        client tun0
                                                                              ip4: 192.168.255.2/24              ip4: 192.168.255.3/24
                                                                              ip6: fc00:0:0:feed::1000/64        ip6: fc00:0:0:feed::1001/64



                                                                                                 Gateway tun0
                                                                                                 ip4: 192.168.255.1/24
                                                                                                 ip6: fc00:0:0:feed::1/64

mtproto-proxy eth0                               bind9-dns eth0                                  openvpn-server eth0                        ipv6nat
ip4: 172.17.0.3/16                               ip4: 172.17.0.2/16                              ip4: 172.17.0.4/16                         ip4: NaN
ip6: fc00:cafe::0:242:ac11:3/32                  ip6: fc00:cafe::0:242:ac11:2/32                 ip6: fc00:cafe::0:242:ac11:4/32            ip6: NaN


                                                     Gateway docker0
                                                     ip4: 172.17.0.1
                                                     ip6: fc00:cafe::1

                                                     VPS eth0
                                                     ip4: 185.22.153.49/23
                                                     ip6: 2a00:b700::6:220/48



                                                        Gateway
                                                        ip4: 185.22.152.1
                                                        ip6: 2a00:b700::1

With this setup its possible to "ping -6 ipv6.google.com" while in "openvpn-server". Packets that originate on
openvpn-server can do a round trip ok, because the vps knows to route the :cafe: network to docker0 bridge.

However we still do not get ipv6 to the internet and back when pinging from a VPN client. The problem is that
the responce packets are trying to get to the host with a VPN only ip, i.e. within the fc00:0:0:feed::/64 network.
Perhaps the solution would be to make a new route in hte vps table to route the responding traffic that is destined
for fc00:0:0:feed::/64 network into the docker openvpn container? Just routing them to the docker0 bridge would
not be enougth.

Better tools are needed to diagnose this problem! Well we have them!
First of all must confirm the point at which the packets are dying. I suspect that they leave the VPN ok, but can not return.

The simplest tool to check connectivity comes to the rescue: socat and nmap (ncat)
Now I will establish connection from my laptop via the OpenVPN server to another vps that I control and that has ipv6 enabled.
Run the commands below while connected to the OpenVPN server.

First check with ipv4:
Client, on the VPN:
$ ncat -4 -u --send-only vps.rinserepeat.site 5000
Server, outside the VPN:
$ sudo socat - udp4-listen:5000,reuseaddr,fork

Then with ipv6:
See https://superuser.com/questions/879498/make-socat-listen-on-both-ipv4-and-ipv6-stacks
Client, on the VPN:
$ ncat -6 -u --send-only vps.rinserepeat.site 5000
Server, outside the VPN:
$ sudo socat - udp6-listen:5000,reuseaddr,fork

Of course to run the server commands you need to ssh to the vps.rinserepeat.site.
An interesting effect is that when the VPN is disconnected the traffic actually gets send through.

Ok, so this will probably break somewhere with ipv6. For me ipv4 worked fine though. Then more details are needed! We need wireshark!
Follow the instructions to stream data for wireshark from the VPN server. Then we can see all traffic going outside and returning.
This will help find the reason.
Unfortunately you must have root login enabled on the remote server for this to work. But its ok, its a test server anyway.
See first answer https://serverfault.com/questions/362529/how-can-i-sniff-the-traffic-of-remote-machine-with-wireshark
In short:

    Create a named pipe:
    $ mkfifo /tmp/remote

    Start wireshark from the command line
    $ wireshark -k -i /tmp/remote

    Run tcpdump over ssh on your remote machine and redirect the packets to the named pipe:
    $ ssh root@firewall "tcpdump -s 0 -U -n -w - -i eth0 not port 22" > /tmp/remote


You will probably need to change the name of the eth0 interface to whatever you have.
And what did I see there in wireshark? Well I saw that traffic was going out to my remote vps.rinserepeat.site (ir ipv6.google.com)
everytime that I typed at the ncat command. But the source address for the traffic was fc00:0:0:feed::1000 so of course the traffic
could never return!

I think that NAT is the best strategy for openvpn tunneling, so I have I have re-installed docker-ipv6nat image back on my server. 
But outgoing traffic still had the VPN local address. Then I looked at the rules for iptables and ip6tables on my vps server.

====================================================
artem@v74891:~$ sudo iptables -t nat --list
[sudo] password for artem:
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
DOCKER     all  --  anywhere            !loopback/8           ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  172.17.0.0/16        anywhere
MASQUERADE  tcp  --  172.17.0.3           172.17.0.3           tcp dpt:webmin
MASQUERADE  tcp  --  172.17.0.3           172.17.0.3           tcp dpt:domain
MASQUERADE  udp  --  172.17.0.3           172.17.0.3           udp dpt:domain
MASQUERADE  tcp  --  172.17.0.4           172.17.0.4           tcp dpt:openvpn
MASQUERADE  udp  --  172.17.0.4           172.17.0.4           udp dpt:openvpn
MASQUERADE  tcp  --  172.17.0.2           172.17.0.2           tcp dpt:https

Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all  --  anywhere             anywhere
DNAT       tcp  --  anywhere             anywhere             tcp dpt:webmin to:172.17.0.3:10000
DNAT       tcp  --  anywhere             anywhere             tcp dpt:domain to:172.17.0.3:53
DNAT       udp  --  anywhere             anywhere             udp dpt:domain to:172.17.0.3:53
DNAT       tcp  --  anywhere             anywhere             tcp dpt:openvpn to:172.17.0.4:1194
DNAT       udp  --  anywhere             anywhere             udp dpt:openvpn to:172.17.0.4:1194
DNAT       tcp  --  anywhere             anywhere             tcp dpt:1010 to:172.17.0.2:443


====================================================
artem@v74891:~$ sudo ip6tables -t nat --list
[sudo] password for artem:
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
DOCKER     all      anywhere             anywhere             ADDRTYPE match dst-type LOCAL

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
DOCKER     all      anywhere            !localhost            ADDRTYPE match dst-type LOCAL

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all      fc00:0:0:cafe::/64   anywhere
MASQUERADE  udp      fc00::cafe:0:242:ac11:3  fc00::cafe:0:242:ac11:3  udp dpt:domain
MASQUERADE  tcp      fc00::cafe:0:242:ac11:3  fc00::cafe:0:242:ac11:3  tcp dpt:webmin
MASQUERADE  tcp      fc00::cafe:0:242:ac11:3  fc00::cafe:0:242:ac11:3  tcp dpt:domain
MASQUERADE  tcp      fc00::cafe:0:242:ac11:2  fc00::cafe:0:242:ac11:2  tcp dpt:webmin
MASQUERADE  tcp      fc00::cafe:0:242:ac11:4  fc00::cafe:0:242:ac11:4  tcp dpt:openvpn
MASQUERADE  tcp      fc00::cafe:0:242:ac11:2  fc00::cafe:0:242:ac11:2  tcp dpt:https
MASQUERADE  udp      fc00::cafe:0:242:ac11:4  fc00::cafe:0:242:ac11:4  udp dpt:openvpn
MASQUERADE  tcp      fc00::cafe:0:242:ac11:2  fc00::cafe:0:242:ac11:2  tcp dpt:domain
MASQUERADE  udp      fc00::cafe:0:242:ac11:2  fc00::cafe:0:242:ac11:2  udp dpt:domain
MASQUERADE  tcp      fc00::cafe:0:242:ac11:3  fc00::cafe:0:242:ac11:3  tcp dpt:https

Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all      anywhere             anywhere
DNAT       tcp      anywhere             anywhere             tcp dpt:webmin to:[fc00::cafe:0:242:ac11:3]:10000
DNAT       tcp      anywhere             anywhere             tcp dpt:domain to:[fc00::cafe:0:242:ac11:3]:53
DNAT       udp      anywhere             anywhere             udp dpt:domain to:[fc00::cafe:0:242:ac11:3]:53
DNAT       tcp      anywhere             anywhere             tcp dpt:openvpn to:[fc00::cafe:0:242:ac11:4]:1194
DNAT       udp      anywhere             anywhere             udp dpt:openvpn to:[fc00::cafe:0:242:ac11:4]:1194
DNAT       tcp      anywhere             anywhere             tcp dpt:1010 to:[fc00::cafe:0:242:ac11:2]:443

So docker ipv6nat was providing NAT for addresses in fc00:0:0:cafe::/64, but the fc00:0:0:feed::/64 was not NATed.
which means that packets went out, they went out with source address in fc00:0:0:feed::/64 and dest address=SOME_IPV6_ADDRESS,
but when the other party wanted to answer it responds by sending the payload to the address in fc00:0:0:feed::/64, because
that was the source address in the . Since this is a Unique Local Address the packets never make it back.
The openvpn docker container had rules to NAT IPv4 as shown below. To get into the container and examine it I
used: "sudo docker exec -i -t openvpn-server /bin/bash"

====================================================
bash-4.4# iptables -t nat --list
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  192.168.255.0/24     anywhere

====================================================
bash-4.4# ip6tables -t nat --list
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination


So the simplest solution is to NAT ipv6 in the container just like ipv4. Then when ipv6 traffic leaves the openvpn
docker container it will appear as if its coming from the fc00:0:0:cafe::/64 network. In which case ipv6nat container
will take care of NATting it again and to the internet it will appear to be coming from the ansible.rinserepeat.site.

So I added a rule inside the docker openvpn container. Command to add the rule is taken from docker-openvpn source code
and here: https://serverfault.com/questions/470169/snat-in-ip6tables

$ ip6tables -t nat -A POSTROUTING -s "fc00:0:0:feed::/64" -o eth0 -j MASQUERADE

Then the status for ip6tables became as below:

====================================================
bash-4.4# ip6tables -t nat --list
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all      fc00:0:0:feed::/64   anywhere

Looks good!
And indeed after testing, it works!
